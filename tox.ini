[tox]
minversion = 4.0
envlist =
    # Lint & Format
    pre-commit
    mypy-{full,test,docs,benchmark_server,micro_benchmarks}
    # Build
    build
    # Tests (3.11)
    py311-other-{tests,docstrings}
    py311-{unit,functional}-{standard,cbor,msgpack}
    py311-functional-{asyncio_proactor,uvloop}
    # Tests (3.12)
    py312-{unit,functional}-{standard,cbor,msgpack}
    py312-functional-{asyncio_proactor,uvloop}
    # Report
    coverage
skip_missing_interpreters = true

[base]
wheel_build_env = {package_env}
setenv =
    PYTHONUNBUFFERED = 1
    PDM_CHECK_UPDATE = False

[docs]
root_dir = {toxinidir}{/}docs
source_dir = {[docs]root_dir}{/}source
extensions_dir = {[docs]source_dir}{/}_extensions
examples_dir = {[docs]source_dir}{/}_include{/}examples

[pytest-conf]
setenv =
    # TODO: setenv substitution does not work for PYTHONHASHSEED (https://github.com/tox-dev/tox/issues/2872)
    # PYTHONHASHSEED = 100
    PYTHONASYNCIODEBUG = 1
addopts = -p "no:cacheprovider" {tty:--color=yes}
unit_tests_rootdir = tests{/}unit_test
functional_tests_rootdir = tests{/}functional_test
cov_opts = --cov --cov-report=''

[testenv:py311-other-{tests,docstrings}]
package = wheel
wheel_build_env = {[base]wheel_build_env}
platform =
    docstrings: linux
groups =
    test
setenv =
    {[base]setenv}
    {[pytest-conf]setenv}
    PYTHONHASHSEED = 0
    PYTEST_ADDOPTS = {[pytest-conf]addopts} --no-cov
commands =
    tests: pytest -m "not unit and not functional" {posargs}
    docstrings: pytest --doctest-modules {posargs} {toxinidir}{/}src
    docstrings: pytest --doctest-modules {posargs} {[docs]examples_dir}{/}tutorials{/}ftp_server
    docstrings: pytest --doctest-glob="*.rst" {posargs} {[docs]source_dir}

[testenv:{py311,py312}-{unit,functional}-{standard,cbor,msgpack}]
package = wheel
wheel_build_env = {[base]wheel_build_env}
groups =
    test
    coverage
    cbor: cbor
    msgpack: msgpack
setenv =
    {[base]setenv}
    {[pytest-conf]setenv}
    PYTHONHASHSEED = 100
    PYTEST_ADDOPTS = {[pytest-conf]addopts} {[pytest-conf]cov_opts}
    COVERAGE_FILE = .coverage.{envname}
    !py311: COVERAGE_CORE=sysmon
    unit: TESTS_ROOTDIR = {[pytest-conf]unit_tests_rootdir}
    functional: TESTS_ROOTDIR = {[pytest-conf]functional_tests_rootdir}
passenv =
    PYTEST_MAX_WORKERS
commands =
    standard: pytest -n "{env:PYTEST_MAX_WORKERS:auto}" -m "not feature" {posargs} {env:TESTS_ROOTDIR}
    cbor: pytest -m "feature_cbor" {posargs} {env:TESTS_ROOTDIR}
    msgpack: pytest -m "feature_msgpack" {posargs} {env:TESTS_ROOTDIR}

[testenv:{py311,py312}-functional-{asyncio_proactor,uvloop}]
package = wheel
wheel_build_env = {[base]wheel_build_env}
platform =
    asyncio_proactor: win32
    uvloop: linux|darwin
groups =
    test
    coverage
    uvloop: uvloop
setenv =
    {[base]setenv}
    {[pytest-conf]setenv}
    PYTHONHASHSEED = 100
    PYTEST_ADDOPTS = {[pytest-conf]addopts} {[pytest-conf]cov_opts}
    COVERAGE_FILE = .coverage.{envname}
    !py311: COVERAGE_CORE=sysmon
    TESTS_ROOTDIR = {[pytest-conf]functional_tests_rootdir}
    asyncio_proactor: ASYNCIO_EVENTLOOP = asyncio-proactor
    uvloop: ASYNCIO_EVENTLOOP = uvloop
passenv =
    PYTEST_MAX_WORKERS
commands =
    pytest -n "{env:PYTEST_MAX_WORKERS:auto}" --asyncio-event-loop="{env:ASYNCIO_EVENTLOOP}" -m "asyncio and not feature" {posargs} {env:TESTS_ROOTDIR}

[testenv:coverage]
skip_install = true
depends =
    {py311,py312}-{unit,functional}-{standard,cbor,msgpack}
    {py311,py312}-functional-{asyncio_proactor,uvloop}
parallel_show_output = True
groups =
    coverage
setenv =
    {[base]setenv}
    COVERAGE_FILE = .coverage
commands_pre =
    coverage erase
commands =
    coverage combine
    coverage report

[testenv:build]
skip_install = true
groups =
    build
setenv =
    {[base]setenv}
passenv =
    SOURCE_DATE_EPOCH
commands =
    python -m build --outdir {toxinidir}{/}dist

[testenv:mypy-{full,test,docs,benchmark_server,micro_benchmarks}]
package = wheel
wheel_build_env = {[base]wheel_build_env}
platform =
    benchmark_server: linux
groups =
    mypy
    test: test
    full,test,micro_benchmarks: cbor
    full,test,micro_benchmarks: msgpack
    full,test,micro_benchmarks: types-msgpack
    docs: doc
    benchmark_server: benchmark-servers
    benchmark_server: benchmark-servers-deps
    micro_benchmarks: micro-benchmark
setenv =
    {[base]setenv}
    MYPY_CACHE_DIR = {envtmpdir}{/}.mypy_cache
    MYPY_OPTS = --config-file {toxinidir}{/}pyproject.toml
commands =
    # package
    full: mypy {env:MYPY_OPTS} -p easynetwork
    # tests
    test: mypy {env:MYPY_OPTS} {toxinidir}{/}tests
    # documentation
    docs: mypy {env:MYPY_OPTS} {[docs]extensions_dir}
    docs: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}tutorials{/}echo_client_server_tcp
    docs: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}tutorials{/}echo_client_server_udp
    docs: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}tutorials{/}ftp_server
    docs: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}protocols
    docs: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}serializers
    docs: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}tcp_clients
    docs: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}tcp_servers
    docs: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}udp_clients
    docs: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}udp_servers
    # benchmark
    benchmark_server: mypy {env:MYPY_OPTS} {toxinidir}{/}benchmark_server
    benchmark_server: mypy {env:MYPY_OPTS} {toxinidir}{/}benchmark_server{/}build_benchmark_image
    benchmark_server: mypy {env:MYPY_OPTS} {toxinidir}{/}benchmark_server{/}run_benchmark
    micro_benchmarks: mypy {env:MYPY_OPTS} {toxinidir}{/}micro_benchmarks

[testenv:pre-commit]
skip_install = true
ignore_outcome = true  # Only trigger a warning on failure.
groups =
    pre-commit
setenv =
    {[base]setenv}
passenv =
    PRE_COMMIT_HOME
    XDG_CACHE_HOME
commands =
    pre-commit run {posargs:--all-files}

[testenv:benchmark-micro]
package = wheel
wheel_build_env = {[base]wheel_build_env}
groups =
    micro-benchmark
    cbor
    msgpack
setenv =
    {[base]setenv}
    {[pytest-conf]setenv}
    PYTHONHASHSEED = 0
    PYTEST_ADDOPTS = {[pytest-conf]addopts}
commands =
    pytest -c pytest-benchmark.ini {posargs:--benchmark-histogram=benchmark_reports{/}micro_benches{/}benchmark}

[testenv:benchmark-server-{tcpecho,sslecho,readline,udpecho}]
skip_install = true
groups =
    benchmark-servers
setenv =
    {[base]setenv}

    # Python version
    BENCHMARK_PYTHON_VERSION = {env:BENCHMARK_PYTHON_VERSION:3.11}

    # Image tag
    BENCHMARK_IMAGE_TAG = easynetwork/benchmark-{env:BENCHMARK_PYTHON_VERSION}

    # Benchmark name
    tcpecho: BENCHMARK_PATTERN = ^tcpecho
    sslecho: BENCHMARK_PATTERN = ^sslecho
    udpecho: BENCHMARK_PATTERN = ^udpecho
    readline: BENCHMARK_PATTERN = ^readline

    # Report files
    BENCHMARK_REPORT_JSON = {toxinidir}{/}benchmark_reports{/}server_benches{/}json{/}{envname}-{env:BENCHMARK_PYTHON_VERSION}-report.json
    BENCHMARK_REPORT_HTML = {toxinidir}{/}benchmark_reports{/}server_benches{/}html{/}{envname}-{env:BENCHMARK_PYTHON_VERSION}-report.html
passenv =
    BENCHMARK_PYTHON_VERSION
    DOCKER_HOST
    DOCKER_CERT_PATH
    DOCKER_TLS_VERIFY
interrupt_timeout = 3.0  # seconds
commands_pre =
    python .{/}benchmark_server{/}build_benchmark_image --tag="{env:BENCHMARK_IMAGE_TAG}" --python-version="{env:BENCHMARK_PYTHON_VERSION}"
commands =
    python .{/}benchmark_server{/}run_benchmark {posargs:--add-date-to-report-file} -J "{env:BENCHMARK_REPORT_JSON}" -H "{env:BENCHMARK_REPORT_HTML}" -b "{env:BENCHMARK_PATTERN}" -t "{env:BENCHMARK_IMAGE_TAG}"

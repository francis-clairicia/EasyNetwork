[tox]
minversion = 4.0
envlist =
    # Lint & Format
    pre-commit
    mypy-{full,test,docs-all,docs-unix,benchmark_server,micro_benchmarks}
    # Build
    build
    doc-html
    # Tests (3.11)
    py311-other-{tests,docstrings}
    py311-{unit,functional}-{standard,cbor,msgpack,trio}
    py311-functional-{asyncio_proactor,uvloop}
    # Tests (3.12)
    py312-{unit,functional}-{standard,cbor,msgpack,trio}
    py312-functional-{asyncio_proactor,uvloop}
    # Tests (3.13)
    py313-{unit,functional}-{standard,cbor,msgpack,trio}
    py313-functional-{asyncio_proactor,uvloop}
    # Report
    coverage-{unit,functional,full}
skip_missing_interpreters = true

[base]
wheel_build_env = {package_env}
setenv =
    PYTHONUNBUFFERED = 1
    PDM_CHECK_UPDATE = False
passenv =
    CI

[docs]
root_dir = {toxinidir}{/}docs
source_dir = {[docs]root_dir}{/}source
build_dir = {[docs]root_dir}{/}build
extensions_dir = {[docs]source_dir}{/}_extensions
examples_dir = {[docs]source_dir}{/}_include{/}examples

[pytest-conf]
setenv =
    # TODO: setenv substitution does not work for PYTHONHASHSEED (https://github.com/tox-dev/tox/issues/2872)
    # PYTHONHASHSEED = 100
    PYTHONASYNCIODEBUG = 1
addopts = -p "no:cacheprovider" {tty:--color=yes} --basetemp="{envtmpdir}"
unit_tests_rootdir = tests{/}unit_test
functional_tests_rootdir = tests{/}functional_test
cov_opts = --cov --cov-report=''

[testenv:py311-other-{tests,docstrings}]
package = wheel
wheel_build_env = {[base]wheel_build_env}
platform =
    docstrings: linux
groups =
    test
    trio  # Needed to import trio backend package
setenv =
    {[base]setenv}
    {[pytest-conf]setenv}
    PYTHONHASHSEED = 0
    PYTEST_ADDOPTS = {[pytest-conf]addopts} --no-cov
passenv =
    {[base]passenv}
commands =
    tests: pytest -m "not unit and not functional" {posargs}
    docstrings: pytest --doctest-modules {posargs} {toxinidir}{/}src
    docstrings: pytest --doctest-modules {posargs} {[docs]examples_dir}{/}tutorials{/}ftp_server
    docstrings: pytest --doctest-glob="*.rst" {posargs} {[docs]source_dir}

[testenv:{py311,py312,py313}-{unit,functional}-{standard,cbor,msgpack,trio}{,-bsd}]
package = wheel
wheel_build_env = {[base]wheel_build_env}
platform =
    !bsd: ((?!bsd).)*
    bsd: (free|open|net)bsd.*
groups =
    test
    ; Tests with SSL are deactivated on BSD because cryptography takes 4 minutes to compile :)
    functional-standard-!bsd: test-ssl
    coverage
    cbor: cbor
    msgpack: msgpack
    trio: trio
    trio: test-trio
setenv =
    {[base]setenv}
    {[pytest-conf]setenv}
    PYTHONHASHSEED = 100
    PYTEST_ADDOPTS = {[pytest-conf]addopts} {[pytest-conf]cov_opts}
    unit: COVERAGE_FILE = .coverage.unit.{envname}
    functional: COVERAGE_FILE = .coverage.functional.{envname}
    !py311: COVERAGE_CORE=sysmon
    unit: TESTS_ROOTDIR = {[pytest-conf]unit_tests_rootdir}
    functional: TESTS_ROOTDIR = {[pytest-conf]functional_tests_rootdir}
passenv =
    {[base]passenv}
    PYTEST_MAX_WORKERS
commands =
    standard: pytest -n "{env:PYTEST_MAX_WORKERS:auto}" -m "not feature" {posargs} {env:TESTS_ROOTDIR}
    cbor: pytest -m "feature_cbor" {posargs} {env:TESTS_ROOTDIR}
    msgpack: pytest -m "feature_msgpack" {posargs} {env:TESTS_ROOTDIR}
    trio: pytest -n "{env:PYTEST_MAX_WORKERS:auto}" -m "feature_trio" {posargs} {env:TESTS_ROOTDIR}

[testenv:{py311,py312,py313}-functional-{asyncio_proactor,uvloop}]
package = wheel
wheel_build_env = {[base]wheel_build_env}
platform =
    asyncio_proactor: win32
    uvloop: linux|darwin
groups =
    test
    test-ssl
    coverage
    uvloop: uvloop
setenv =
    {[base]setenv}
    {[pytest-conf]setenv}
    PYTHONHASHSEED = 100
    PYTEST_ADDOPTS = {[pytest-conf]addopts} {[pytest-conf]cov_opts}
    COVERAGE_FILE = .coverage.functional.{envname}
    !py311: COVERAGE_CORE=sysmon
    TESTS_ROOTDIR = {[pytest-conf]functional_tests_rootdir}
    asyncio_proactor: ASYNCIO_EVENTLOOP = asyncio-proactor
    uvloop: ASYNCIO_EVENTLOOP = uvloop
passenv =
    {[base]passenv}
    PYTEST_MAX_WORKERS
commands =
    pytest -n "{env:PYTEST_MAX_WORKERS:auto}" --asyncio-event-loop="{env:ASYNCIO_EVENTLOOP}" -m "asyncio and not feature" {posargs} {env:TESTS_ROOTDIR}

[testenv:coverage-{unit,functional,full}]
skip_install = true
depends =
    unit: {py311,py312,py313}-unit-{standard,cbor,msgpack,trio}{,-bsd}
    functional: {py311,py312,py313}-functional-{standard,cbor,msgpack,trio}{,-bsd}
    functional: {py311,py312,py313}-functional-{asyncio_proactor,uvloop}
    full: coverage-{unit,functional}
parallel_show_output =
    full: True
groups =
    coverage
setenv =
    {[base]setenv}
    unit: COVERAGE_FILE = .coverage.unit
    functional: COVERAGE_FILE = .coverage.functional
    full: COVERAGE_FILE = .coverage
passenv =
    {[base]passenv}
commands =
    !full: coverage combine
    full: coverage combine --keep
    coverage report
    unit: coverage xml -o coverage.unit.xml
    functional: coverage xml -o coverage.functional.xml
    full: coverage xml -o coverage.xml

[testenv:build]
skip_install = true
groups =
    build
setenv =
    {[base]setenv}
passenv =
    {[base]passenv}
    SOURCE_DATE_EPOCH
commands =
    python -m build --outdir {toxinidir}{/}dist

[testenv:doc-html]
package = editable
change_dir = {[docs]root_dir}
groups =
    doc
setenv =
    {[base]setenv}
    SOURCEDIR = source
    BUILDDIR = build
passenv =
    {[base]passenv}
commands =
    sphinx-build -T -b html {env:SOURCEDIR} {env:BUILDDIR} {posargs}

[testenv:mypy-{full,test,docs-all,docs-unix,benchmark_server,micro_benchmarks}]
package = wheel
wheel_build_env = {[base]wheel_build_env}
platform =
    benchmark_server: linux
    docs-unix: linux|darwin|(free|open|net)bsd.*
groups =
    mypy
    test: test
    test: test-ssl
    full,test,micro_benchmarks: cbor
    full,test,micro_benchmarks: msgpack
    full,test,micro_benchmarks: types-msgpack
    full,test,docs: trio
    docs: doc
    benchmark_server: benchmark-servers
    benchmark_server: benchmark-servers-deps
    micro_benchmarks: micro-benchmark
setenv =
    {[base]setenv}
    MYPY_CACHE_DIR = {envtmpdir}{/}.mypy_cache
    MYPY_OPTS = --config-file {toxinidir}{/}pyproject.toml
passenv =
    {[base]passenv}
commands =
    # package
    full: mypy {env:MYPY_OPTS} -p easynetwork
    # tests
    test: mypy {env:MYPY_OPTS} {toxinidir}{/}tests
    # documentation
    docs-all: mypy {env:MYPY_OPTS} {[docs]extensions_dir}
    docs-all: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}tutorials{/}echo_client_server_tcp
    docs-all: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}tutorials{/}echo_client_server_udp
    docs-all: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}tutorials{/}ftp_server
    docs-all: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}protocols
    docs-all: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}serializers
    docs-all: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}tcp_clients
    docs-all: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}tcp_servers
    docs-all: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}udp_clients
    docs-all: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}howto{/}udp_servers
    docs-unix: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}alternatives{/}unix_datagram_clients
    docs-unix: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}alternatives{/}unix_datagram_servers
    docs-unix: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}alternatives{/}unix_stream_clients
    docs-unix: mypy {env:MYPY_OPTS} {[docs]examples_dir}{/}alternatives{/}unix_stream_servers
    # benchmark
    benchmark_server: mypy {env:MYPY_OPTS} {toxinidir}{/}benchmark_server
    benchmark_server: mypy {env:MYPY_OPTS} {toxinidir}{/}benchmark_server{/}build_benchmark_image
    benchmark_server: mypy {env:MYPY_OPTS} {toxinidir}{/}benchmark_server{/}run_benchmark
    micro_benchmarks: mypy {env:MYPY_OPTS} {toxinidir}{/}micro_benchmarks

[testenv:pre-commit]
skip_install = true
ignore_outcome = true  # Only trigger a warning on failure.
groups =
    pre-commit
setenv =
    {[base]setenv}
passenv =
    {[base]passenv}
    PRE_COMMIT_HOME
    XDG_CACHE_HOME
commands =
    pre-commit run {posargs:--all-files}

[testenv:benchmark-micro]
package = wheel
wheel_build_env = {[base]wheel_build_env}
groups =
    micro-benchmark
    cbor
    msgpack
setenv =
    {[base]setenv}
    {[pytest-conf]setenv}
    PYTHONHASHSEED = 0
    PYTEST_ADDOPTS = {[pytest-conf]addopts}
passenv =
    {[base]passenv}
commands =
    pytest -c pytest-benchmark.ini {posargs:--benchmark-histogram=benchmark_reports{/}micro_benches{/}benchmark}

[testenv:benchmark-server-{tcpecho,sslecho,unixstreamecho,readline_tcp,readline_unix,udpecho,unixdgramecho}-{easynetwork,stdlib}]
skip_install = true
groups =
    benchmark-servers
setenv =
    {[base]setenv}

    # Python version
    BENCHMARK_PYTHON_VERSION = {env:BENCHMARK_PYTHON_VERSION:3.11}

    # Image tag
    BENCHMARK_IMAGE_TAG = easynetwork/benchmark-{env:BENCHMARK_PYTHON_VERSION}

    # Benchmark name
    ## TCP echo
    tcpecho-easynetwork: BENCHMARK_PATTERN = ^tcpecho-easynetwork
    tcpecho-stdlib: BENCHMARK_PATTERN = ^tcpecho-(?!easynetwork)
    ## SSL echo
    sslecho-easynetwork: BENCHMARK_PATTERN = ^sslecho-easynetwork
    sslecho-stdlib: BENCHMARK_PATTERN = ^sslecho-(?!easynetwork)
    ## UNIX stream echo
    unixstreamecho-easynetwork: BENCHMARK_PATTERN = ^unixstreamecho-easynetwork
    unixstreamecho-stdlib: BENCHMARK_PATTERN = ^unixstreamecho-(?!easynetwork)
    ## UDP echo
    udpecho-easynetwork: BENCHMARK_PATTERN = ^udpecho-easynetwork
    udpecho-stdlib: BENCHMARK_PATTERN = ^udpecho-(?!easynetwork)
    ## UNIX datagram echo
    unixdgramecho-easynetwork: BENCHMARK_PATTERN = ^unixdgramecho-easynetwork
    unixdgramecho-stdlib: BENCHMARK_PATTERN = ^unixdgramecho-(?!easynetwork)
    ## TCP readline
    readline_tcp-easynetwork: BENCHMARK_PATTERN = ^readline-tcp-easynetwork
    readline_tcp-stdlib: BENCHMARK_PATTERN = ^readline-tcp-(?!easynetwork)
    ## UNIX readline
    readline_unix-easynetwork: BENCHMARK_PATTERN = ^readline-unixstream-easynetwork
    readline_unix-stdlib: BENCHMARK_PATTERN = ^readline-unixstream-(?!easynetwork)

    # Report files
    BENCHMARK_REPORT_JSON = {toxinidir}{/}benchmark_reports{/}server_benches{/}json{/}{envname}-{env:BENCHMARK_PYTHON_VERSION}-report.json
    BENCHMARK_REPORT_HTML = {toxinidir}{/}benchmark_reports{/}server_benches{/}html{/}{envname}-{env:BENCHMARK_PYTHON_VERSION}-report.html
passenv =
    {[base]passenv}
    BENCHMARK_PYTHON_VERSION
    DOCKER_HOST
    DOCKER_CERT_PATH
    DOCKER_TLS_VERIFY
interrupt_timeout = 3.0  # seconds
commands_pre =
    python .{/}benchmark_server{/}build_benchmark_image --tag="{env:BENCHMARK_IMAGE_TAG}" --python-version="{env:BENCHMARK_PYTHON_VERSION}"
commands =
    # For easynetwork benchmarks, also dump in JSON format
    easynetwork: python .{/}benchmark_server{/}run_benchmark {posargs:--add-date-to-report-file} -J "{env:BENCHMARK_REPORT_JSON}" -H "{env:BENCHMARK_REPORT_HTML}" -b "{env:BENCHMARK_PATTERN}" -t "{env:BENCHMARK_IMAGE_TAG}"
    # For other benchmarks, only create HTML report
    !easynetwork: python .{/}benchmark_server{/}run_benchmark {posargs:--add-date-to-report-file} -H "{env:BENCHMARK_REPORT_HTML}" -b "{env:BENCHMARK_PATTERN}" -t "{env:BENCHMARK_IMAGE_TAG}"
